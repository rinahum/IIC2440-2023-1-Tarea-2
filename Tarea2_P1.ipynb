{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Tarea 2\n","## IIC2440 - Procesamiento de Datos Masivos\n","\n","Integrantes:\n","- Rodrigo Nahum\n","- Fernando Quintana\n","\n","## Parte 2: Single Source Shortest Path"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup\n","\n","Primero, instalamos pyspark"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47607,"status":"ok","timestamp":1687545407042,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"RyojxTk_3kGi","outputId":"0233b0a7-2c94-4c9a-9d0a-5fbac7f4d938"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=0b842c6cb2b7d735a84638ba5b1407cdc8ee5e5e806ea392d028e78560d7e364\n","  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.1\n"]}],"source":["!pip install pyspark"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Importamos PySpark y creamos un Spark Context"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9656,"status":"ok","timestamp":1687545416695,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"GTGWCKe13mfZ"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .getOrCreate()\n","\n","sc = spark.sparkContext"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xqILLK933pcw"},"source":["# Input\n","\n","Definimos un grafo de ejemplo a usar, con una cantidad de nodos y una probabilidad de que cada arista exista"]},{"cell_type":"code","execution_count":97,"metadata":{"executionInfo":{"elapsed":69022,"status":"ok","timestamp":1687549958862,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"YtN8qu4xyQ2q"},"outputs":[],"source":["from itertools import permutations\n","import random\n","import json\n","\n","n = 10000\n","\n","edge_chance = 0.1\n","\n","node_list = [i + 1 for i in range(n)]\n","\n","edge_list = []\n","for edge in permutations(node_list, 2):\n","    if random.random() < edge_chance:\n","        edge_list.append(edge)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Definimos el damping factor y pasamos a RDD las listas de nodos y aristas"]},{"cell_type":"code","execution_count":107,"metadata":{"executionInfo":{"elapsed":3076,"status":"ok","timestamp":1687550694170,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"T8d51MqO3oT6"},"outputs":[],"source":["damping_factor = 0.85\n","\n","nodes = sc.parallelize(node_list)\n","node_count = nodes.count()\n","edges = sc.parallelize(edge_list)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Inicializacion de nodos\n","\n","Inicializamos los nodos para Page Rank, seteando sus valores iniciales 1 divido en la cantidad de nodos.\n","\n","Ademas calculamos la cantidad de aristas que tiene cada nodo, de forma de setear que porcentaje del valor del nodo sale por cada arista."]},{"cell_type":"code","execution_count":69,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1687548973847,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"CiNslIZK4S2o"},"outputs":[],"source":["initial_nodes = nodes.map(lambda node: (node, 1/node_count))\n","edge_count = edges.map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x + y).map(lambda x: (x[0], 1/x[1]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Generacion de mensajes\n","\n","Para crear los mensajes tomamos los nodos y los unimos con las aristas, tomando el valor actual del nodo y multiplicandolo por el 1/(cantidad de aristas del nodo) las cuales se encuentran en edge_count. Esto nos entrega cuanto del valor actual del nodo va a salir a cada arista que sale de ese nodo"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1687548984191,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"Mmy3nVEYBwhw"},"outputs":[],"source":["messages = initial_nodes.join(edge_count).map(lambda x: (x[0], x[1][0] * x[1][1]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Luego tomamos estos mensajes y los unimos con las aristas, obteniendo cuanto llega a cada arista."]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2019,"status":"ok","timestamp":1687548963791,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"9MZBSBxjCsco","outputId":"e35cc0f7-11e9-45a7-928e-24422e959c42"},"outputs":[{"data":{"text/plain":["[(1, (0.25, 2)), (2, (0.125, 3)), (2, (0.125, 4)), (3, (0.25, 2))]"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["exchange = messages.join(edges)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Filtrado de Mensajes\n","\n","En page Rank no filtramos los mensajes"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Agregacion de mensajes "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Despues de tener los valores que llegan a cada arista lo que hacemos es un map, para dejar solo el nodo y el valor que recibe y luego un reduceByKey donde vamos sumando todos los valores que recibe el nodo desde sus aristas"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3874,"status":"ok","timestamp":1687549064702,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"4KbfWJJ-wjnD","outputId":"c63aa731-1117-4813-ca10-bddc8ead0901"},"outputs":[{"data":{"text/plain":["[(2, 0.5), (3, 0.125), (4, 0.125)]"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["exchange_reduce = exchange.map(lambda x: (x[1][1], x[1][0])).reduceByKey(lambda x, y: x + y)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ademas calculamos el total page rank, el cual nos servira mas adelante para hacer una normalizacion sobre el page rank total"]},{"cell_type":"code","execution_count":77,"metadata":{"executionInfo":{"elapsed":842,"status":"ok","timestamp":1687549052411,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"dAq-AmmNwnpI"},"outputs":[],"source":["total_page_rank = exchange_reduce.map(lambda x: x[1]).reduce(lambda x, y : x + y)"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":893,"status":"ok","timestamp":1687549056402,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"QAkpOYI8wqHa","outputId":"6f61c2e5-256c-4ffa-c218-5784c7677eae"},"outputs":[{"data":{"text/plain":["[(2, 0.6666666666666666), (3, 0.16666666666666666), (4, 0.16666666666666666)]"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["exchange_reduce = exchange_reduce.map(lambda x: (x[0], x[1] / total_page_rank))"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1687545588838,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"DaAkRkE8OTtv"},"outputs":[],"source":["def compute_final_page_rank(x):\n","  if(x[1][1] is None):\n","    return (x[0], (1 - damping_factor)/node_count)\n","  return (x[0], x[1][1]*damping_factor + (1 - damping_factor)/node_count)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Update del estado\n","\n","Para updatear el estado hacemos un leftOuterJoin(ya que un nodo podria no tener aristas entrantes) y calculamos el nuevo page rank aplicando el damping factor con compute_final_page_rank"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2758,"status":"ok","timestamp":1687548537282,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"z8GPNDn8OKjp","outputId":"19c95e72-6bd2-4e70-e879-f60055f080ca"},"outputs":[{"data":{"text/plain":["[(1, 0.037500000000000006),\n"," (2, 0.6041666666666666),\n"," (3, 0.17916666666666667),\n"," (4, 0.17916666666666667)]"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["final_page_rank = initial_nodes.leftOuterJoin(exchange_reduce).map(compute_final_page_rank)\n","final_page_rank.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Por ultimo se define una funcion que permite calcular page rank con n iteraciones entregando los nodos como un RDD, los edges con un RDD y el numero de iteraciones."]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":184518,"status":"ok","timestamp":1687550882497,"user":{"displayName":"Rodrigo Nahum","userId":"03822543747215429537"},"user_tz":240},"id":"4uK99T-DPWqa","outputId":"8514cd07-b729-470b-f360-aefc74f88e21"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n"]}],"source":["def compute_final_page_rank(x):\n","  if(x[1][1] is None):\n","    return (x[0], (1 - damping_factor)/node_count)\n","  return (x[0], x[1][1]*damping_factor + (1 - damping_factor)/node_count)\n","\n","sc.setCheckpointDir(\"/content/checkpoints\")\n","def computePageRank(nodes, edges, iter):\n","\n","  node_count = nodes.count()\n","\n","  initial_nodes = nodes.map(lambda node: (node, 1/node_count)).cache()\n","  edge_count = edges.map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x + y).map(lambda x: (x[0], 1/x[1])).cache()\n","  nodes = initial_nodes\n","\n","  for i in range(iter):\n","    print(i)\n","    messages = nodes.join(edge_count, 8).map(lambda x: (x[0], x[1][0] * x[1][1]))\n","\n","    exchange = messages.join(edges, 8)\n","\n","    exchange_reduce = exchange.map(lambda x: (x[1][1], x[1][0])).reduceByKey(lambda x, y: x + y).cache()\n","    total_page_rank = exchange_reduce.map(lambda x: x[1]).reduce(lambda x, y : x + y)\n","    exchange_reduce = exchange_reduce.map(lambda x: (x[0], x[1] / total_page_rank))\n","\n","    nodes = initial_nodes.leftOuterJoin(exchange_reduce, 8).map(compute_final_page_rank).cache()\n","\n","  return nodes.collect()\n","\n","page_rank = computePageRank(nodes, edges, 3)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
